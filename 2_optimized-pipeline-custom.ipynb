{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monomer-optimized AlphaFold Inference pipeline \n",
    "\n",
    "This notebook demonstrates how to run a Monomer-optimized AlphaFold Inference pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U kfp google-cloud-aiplatform google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the values of the following parameters to reflect your environment.\n",
    "\n",
    "- `PROJECT` - Project ID of your environment\n",
    "- `REGION`- GCP Region where your resources are located\n",
    "- `BUCKET_NAME` - GCS bucket to use for Vertex staging. Must be located in the `REGION`\n",
    "- `FILESTORE_IP` - IP address of your Filestore instance\n",
    "- `FILESTORE_NETWORK_NAME` - Filestore VPC name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = '<YOUR PROJECT ID>'  # Change to your project ID\n",
    "REGION = '<YOUR REGION>'   # Change to your region\n",
    "BUCKET_NAME = '<YOUR BUCKET NAME>'  # Change to your bucket name        \n",
    "FILESTORE_IP = '<FILESTORE IP ADDREDD>' # Change the IP of your Filestore instance\n",
    "FILESTORE_NETWORK_NAME = '<FILESTORE NETWORK NAME>' # Change to the name of the Filestore instance network name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set up the sandbox environment using the provided Terraform configuration you do not need to change the below settings. Otherwise make sure that they are consistent with your environment.\n",
    "\n",
    "- `FILESTORE_SHARE` - Filestore share with AlphaFold reference databases\n",
    "- `FILESTORE_MOUNT_PATH` - Mount path for Filestore fileshare\n",
    "- `MODEL_PARAMS` - GCS location of AlphaFold model parameters. The pipelines are configured to retrieve the parameters from the `<MODEL_PARAMS>/params` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILESTORE_SHARE = '/datasets'\n",
    "FILESTORE_MOUNT_PATH = '/mnt/nfs/alphafold'   \n",
    "PROJECT_NUMBER = !(gcloud projects list --filter=\"projectId:{PROJECT}\" --format=\"value(PROJECT_NUMBER)\")  \n",
    "FILESTORE_NETWORK = f'projects/{PROJECT_NUMBER[0]}/global/networks/{FILESTORE_NETWORK_NAME}'  \n",
    "MODEL_PARAMS = f'gs://{BUCKET_NAME}'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT}/alphafold-components'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and run the Optimized pipeline with custom settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of parameters that can be used to customize Vertex Pipelines: compile time and runtime. The compile time parameters must be set before compiling the pipeline code. The runtime parameters can be supplied when starting a pipeline run.\n",
    "\n",
    "In the AlphaFold inference pipelines, the compile time parameters are used to control settings like CPU/GPU configuration of compute nodes and the Filestore instance settings. The runtime parameters include a sequence to fold, model presets, the maximum date for template searches and more. \n",
    "\n",
    "The pipelines have been designed to retrieve compile time parameters from environment variables. This makes it easy to integrate a pipeline compilation step with CI/CD systems.\n",
    "\n",
    "By default, the pipeline uses a `c2-standard-16` node to run the feature engineering step and  `n1-standard-8` nodes with NVIDIA T4 GPUs to run prediction and relaxation.  This hardware configuration is optimal for folding smaller proteins, roughly 1000 residues or fewer. \n",
    "\n",
    "To demonstrate how you can change the default pipeline settings, you will reconfigure the pipeline to use nodes with a single NVIDIA A100 GPU for prediction and relaxation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set compile time parameters\n",
    "\n",
    "At minimum you have to configure:\n",
    "- the settings of your Filestore instance that hosts genetic databases, \n",
    "- the URI of the docker image that packages custom KFP components, and \n",
    "- the GCS location of AlphaFold parameters.\n",
    "\n",
    "The other variables set in the below cell configure hardware settings for prediction and relaxation nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ALPHAFOLD_COMPONENTS_IMAGE'] = IMAGE_URI\n",
    "os.environ['NFS_SERVER'] = FILESTORE_IP\n",
    "os.environ['NFS_PATH'] = FILESTORE_SHARE\n",
    "os.environ['NETWORK'] = FILESTORE_NETWORK\n",
    "os.environ['MODEL_PARAMS_GCS_LOCATION'] = MODEL_PARAMS\n",
    "\n",
    "# Host configuration for Inference\n",
    "os.environ['MEMORY_LIMIT'] = '85'    # Amount of host memory (RAM)\n",
    "os.environ['CPU_LIMIT'] = '12'       # Number of CPUs\n",
    "os.environ['GPU_LIMIT'] = '1'        # Number of GPUs\n",
    "os.environ['GPU_TYPE'] = 'nvidia-tesla-a100'  # GPU type\n",
    "\n",
    "# Host configuration for Protein Relaxation\n",
    "os.environ['RELAX_MEMORY_LIMIT'] = '85'    # Amount of host memory (RAM)\n",
    "os.environ['RELAX_CPU_LIMIT'] = '12'       # Number of CPUs\n",
    "os.environ['RELAX_GPU_LIMIT'] = '1'        # Number of GPUs\n",
    "os.environ['RELAX_GPU_TYPE'] = 'nvidia-tesla-a100'  # GPU type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.alphafold_optimized_monomer import alphafold_monomer_pipeline as pipeline\n",
    "\n",
    "pipeline_name = 'monomer-optimized-pipeline'\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=f'{pipeline_name}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure runtime parameters\n",
    "\n",
    "At minimum you need to configure a GCS location of your sequence, the maximum date for template searches and a project and region where to run the pipeline. With the default settings, the pipeline will run inference using the full version of BFD. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy the sample sequence to a GCS location\n",
    "\n",
    "You can find a few sample sequences in the `sequences` folder.\n",
    "\n",
    "For this demo, we will use the `Q9Y490`, which is a medium size sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = 'Q9Y490.fasta'\n",
    "gcs_sequence_path = f'gs://{BUCKET_NAME}/fasta/{sequence}'\n",
    "\n",
    "! gsutil cp sequences/{sequence} {gcs_sequence_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'sequence_path': gcs_sequence_path,\n",
    "    'max_template_date': '2020-05-14',\n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'is_run_relax': 'relax'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a pipeline run\n",
    "\n",
    "We recommend annotating pipeline runs with at least two labels. The first label groups multiple pipeline runs into a single experiment. The second label identifies a given run within the experiment. Annotating with labels helps to discover and analyze pipeline runs in large scale settings. The third notebook that demonstrates how to analyze pipeline runs depends on the labels. \n",
    "\n",
    "You will be able to monitor the run using the link printed by executing the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=f'gs://{BUCKET_NAME}/staging'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of your experiment\n",
    "# This name will be used to locale the PipelineJob\n",
    "experiment_id = 'Q9Y490-experiment'\n",
    "labels = {'experiment_id': experiment_id.lower(), 'sequence_id': sequence.split(sep='.')[0].lower()}\n",
    "\n",
    "pipeline_job = vertex_ai.PipelineJob(\n",
    "    display_name=pipeline_name,\n",
    "    template_path=f'{pipeline_name}.json',\n",
    "    pipeline_root=f'gs://{BUCKET_NAME}/pipeline_runs/{pipeline_name}',\n",
    "    parameter_values=params,\n",
    "    enable_caching=False,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "pipeline_job.run(sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the state of the pipeline\n",
    "pipeline_job.state"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m92",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m92"
  },
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
